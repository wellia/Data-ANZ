 
Wellia Lioeng

```{r}

# remove memory in R
# rm(list = ls())
# 
# .rs.restartR()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

```{r}
packages <- c("tidyverse", "readxl", "lubridate", "leaflet", "geosphere", "sp", "mgcv", "fitdistrplus", "gamlss", "rpart.plot")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}

library(lubridate)
library(tidyverse) 
library(readxl) 
library(sp)
library(leaflet)
library(geosphere)
library(mgcv)
library(fitdistrplus)
library(gamlss)
library(dplyr)
library(rpart)
library(rpart.plot)

```
MODULE 1

1. Data exploration

```{r}
file_name = 'ANZ synthesised transaction dataset.xlsx'
nms <- names(read_excel(file_name, n_max = 0))
ct <- ifelse(grepl("bpay_biller_code", nms), "text", "guess")
df <- read_excel(file_name, col_types = ct)

dim(df) # 12043 transactions

length(unique(df$customer_id)) # 100 customers

# confirm the one -to -one link of account_id and customer_id
df %>% dplyr::select(account,customer_id) %>%
  unique() %>%
  nrow()

# check NA
lapply(df,function(x) { length(which(is.na(x)))}) # there are some NA, but it's ok

# check column types 
sapply(df, class) # column types are ok

# convert to date type for easy manipulation
df$trans_date <- as.Date(df$date)

range(df$trans_date)  # data range 01 Aug 2018 to 31 Oct 2018

# check missing dates
start_date <- '2018-08-01'
end_date <- '2018-10-31'
date_range <- seq(as.Date(start_date), as.Date(end_date), by='days')
date_range[!date_range %in% df$trans_date] # missing 16 Aug 2018

unique(df$country) # all customers are from Australia

# split customer & merchant lat_long into individual columns for analysis
dfloc = df[,c("long_lat","merchant_long_lat")]
dfloc<- dfloc %>% separate("long_lat", c("c_long", "c_lat"),sep=' ')
dfloc<- dfloc %>% separate("merchant_long_lat", c("m_long", "m_lat"),sep=' ')
dfloc<- data.frame(sapply(dfloc, as.numeric))
df <- cbind(df,dfloc)

# check the range of customer location
# filtering out transactions for those who don't reside in Australia
# Reference: http://www.ga.gov.au/scientific-topics/national-location-information/dimensions/continental-extremities
df_temp <- df %>%
 filter (!(c_long >113 & c_long <154 & c_lat > (-44) & c_lat < (-10)))
length(unique(df_temp$customer_id))


```

```{r}
unique(df$txn_description)

# include purchase transactions only

# assuming purchase transactions must be associated with a merchant (have a merchant Id)
df_temp <- df %>% filter(merchant_id != '' )

# it turned out that is equivilent to excluding following categories of transactions
df_csmp <- df %>%filter(!(txn_description %in% c('PAY/SALARY',"INTER BANK", "PHONE BANK","PAYMEN
T")))

summary(df_csmp)
```

```{r}
# visualise the distribution of transaction amount
amount_outliers <- boxplot.stats(df_csmp$amount)$out
df_csmp_outliers <- df_csmp$amount %in% amount_outliers
hist(df_csmp$amount[!df_csmp_outliers], xlab= 'Transaction Amount', main = 'Histogram of purchase transaction amount')

hist(df$amount[!df$amount %in% boxplot.stats(df$amount)$out], 
     xlab= 'Transaction Amount',main = 'Histogram of overall transaction amount')
```
Visualise customersâ€™average monthly transaction volume.


```{r}

df2 <- df %>%
  group_by(customer_id) %>%
  summarise(mon_avg_vol = round(n()/3,0))
  hist(df2$mon_avg_vol,
    xlab= 'Monthly transaction volume', ylab='No. of customers', 
    main = "Histogram of customers' monthly transaction volume")
```

1.4 Segment the dataset by transaction date and time.

```{r}
df$extraction = as.character(df$extraction)
df$hour = hour(as.POSIXct(substr(df$extraction,12,19),format="%H:%M:%S"))

# Visualise transaction volume over an average week.
df$wk <- factor(wday(df$trans_date, label=TRUE), ordered = TRUE)

df3 <- df %>%
  dplyr::select(date,wk) %>%
  group_by(date,wk) %>%
  summarise(daily_avg_vol = n()) %>%
  group_by(wk) %>%
  summarise(avg_vol=mean(daily_avg_vol,na.rm=TRUE ))

ggplot(df3,aes(x=wk, y=avg_vol)) +geom_point()+geom_line(aes(group = 1))+
  ggtitle('Average transaction volume by weekday') +
  labs(x='Weekday',y='Transaction volume')
```
```{r}
df4 <- df %>%
  dplyr::select(trans_date,hour) %>%
  group_by(trans_date,hour) %>%
  summarize(trans_vol=n()) %>%
  group_by(hour) %>%
  summarize(trans_vol_per_hr = mean(trans_vol,na.rm=TRUE))
  ggplot(df4,aes(x=hour,y=trans_vol_per_hr))+geom_point()+geom_line(aes(group = 1))+
  ggtitle('Average transaction volume by hour') +
  labs(x='Hour',y='Transaction volume') + expand_limits( y = 0)
```

1.5 challenge: exploring location information
We could firstly see the distribution of distance between a customer and the merchange he/she trades with.

```{r}
# exclude the single foreign customer whose location information was incorrectly stored (i.e latitude 573)
df_temp <- df_csmp %>%
  filter (c_long >113 & c_long <154 & c_lat > (-44) & c_lat < (-10))

dfloc = df_temp [,c("c_long", "c_lat","m_long", "m_lat")]
dfloc<- data.frame(sapply(dfloc, as.numeric))

dfloc$dst <- distHaversine(dfloc[, 1:2], dfloc[, 3:4]) / 1000

hist(dfloc$dst[dfloc$dst<100], main = "Distance between customer and merchants",xlab= 'Distance
(km)' )
```

To validate, we could further plot the location of the customer and the merchants he/she trades with on a map.

```{r}
merch_dist <- function (id ){
  ### This function takes in a customer Id and plot the location of the customer and all
  ### merchants he/she have traded with.
  cus_icon<- makeAwesomeIcon(icon = 'home', markerColor = 'green')
  l = subset (df_csmp[,c("customer_id","m_long","m_lat")], customer_id == id)
  l <- l[c("m_long","m_lat")]
  cus_loc <- unique(subset (df_csmp[,c("customer_id","long_lat")], customer_id == id))
  cus_loc <- cus_loc %>% separate("long_lat", c("long", "lat"),sep=' ')
  df_t = data.frame(longtitude = as.numeric(l$m_long), latitude = as.numeric(l$m_lat))
  df_t <- na.omit(df_t)
  coordinates(df_t) <- ~longtitude+latitude
  leaflet(df_t) %>% addMarkers() %>% addTiles() %>%
    addAwesomeMarkers(
      lng=as.numeric(cus_loc$long), lat=as.numeric(cus_loc$lat),
      icon = cus_icon)
}
merch_dist(id ='CUS-51506836' )
```

MODULE 2

```{r}
# identify the annual salary for each customer

df_salary <- df %>%filter(txn_description %in% c('PAY/SALARY'))

df_salary_customer <- df_salary %>%
  group_by(customer_id) %>%
  summarise(annual_salary = mean(amount) * 12)

df_salary_customer %>% filter(annual_salary <= 0) # no customers with 0 salary

hist(df_salary_customer$annual_salary, xlab= 'Annual Salary', ylab = 'count', main = 'Histogram of annual salary per customer')


```


Explore correlations between annual salary and various customer attributes (e.g. age). These attributes could be those that are readily available in the data (e.g. age) or those that you construct or derive yourself (e.g. those relating to purchasing behaviour). Visualise any interesting correlations using a scatter plot.

```{r}

# merge tables
df_salary_customer2 <- df_salary_customer %>% 
  dplyr::inner_join(dplyr::distinct(df[,c('customer_id', 'age', 'long_lat')])) 

# relation between age and salary
df_salary_customer2 %>% 
  ggplot() + 
  geom_point(mapping = aes(x = age, y = annual_salary))
# there is relationship between age and annual_salary but they are not linear. 
# most salary are in age 20 to 50
# high salary (>50000) are mostly between 25 to 40

# calculate customer spending
df_customer_spending <- df %>%
  group_by(customer_id) %>%
  summarise(customer_spending = sum(amount))

sum(is.na(df_salary_customer2$annual_salary)) # no na salary

# merge tables
df_salary_customer3 <- merge(df_salary_customer2, df_customer_spending[, c("customer_id", "customer_spending")])

# relation between spending and age
df_salary_customer3 %>% 
  ggplot() + 
  geom_point(mapping = aes(x = customer_spending, y = annual_salary))
# They have non linear relationship. People who have medium range salary (between $2500 to $7500) tends to spend more

# add location
df_salary_customer4 <- df_salary_customer3 %>% separate("long_lat", c("c_long", "c_lat"),sep=' ')

```


Build a simple regression model to predict the annual salary for each customer using the attributes you identified above

Find the type of distribution
```{r}
df_salary_customer %>% 
  ggplot(aes(x = annual_salary)) +
  geom_histogram()
# result: It does not look like normal distribution

```


```{r}

# build 

# fit gaussian
gam_model <- gam(annual_salary ~ s(age) + s(customer_spending) + s(as.numeric(c_lat)) + s(as.numeric(c_long)), data=df_salary_customer4, method = "REML")
summary(gam_model)
gam.check(gam_model)

# fit log normal
gam_model1 <- gamlss(annual_salary~ cs(age) + cs(customer_spending) + cs(as.numeric(c_lat)) + cs(as.numeric(c_long)), family=LOGNO, data=df_salary_customer4) #fits the log-Normal distribution
plot(gam_model1)
summary(gam_model1)
 

```


How accurate is your model? Should ANZ use it to segment customers (for whom it does not have this data) into income brackets for reporting purposes?

My model only can explain 28% of the response variable. The residual

For the purpose of building model with decision-tree, the dependent variable should be divided into income brackets

```{r}
df_salary_customer4$income_range[df_salary_customer4$annual_salary < 10000] <- 0
df_salary_customer4$income_range[df_salary_customer4$annual_salary >= 10000 & df_salary_customer4$annual_salary < 30000] <- 1
df_salary_customer4$income_range[df_salary_customer4$annual_salary >= 30000 & df_salary_customer4$annual_salary < 60000] <- 2
df_salary_customer4$income_range[df_salary_customer4$annual_salary >= 60000 & df_salary_customer4$annual_salary < 90000] <- 3
df_salary_customer4$income_range[df_salary_customer4$annual_salary >= 90000] <- 4

df_salary_customer4$income_range <- factor(df_salary_customer4$income_range)

```

For a challenge: build a decision-tree based model to predict salary. Does it perform better? How would you accurately test the performance of this model?

```{r}
create_train_test <- function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}

data_train <- create_train_test(df_salary_customer4, 0.8, train = TRUE)
data_test <- create_train_test(df_salary_customer4, 0.8, train = FALSE)
dim(df_salary_customer4)

fit <- rpart(salary_range~., data = data_train, method = 'class')
rpart.plot(fit, extra = 106)

```

